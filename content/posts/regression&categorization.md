---
title: 回归（Regression）与 分类（Categorization）
date: 2019-11-15 17:31:56
tags: [Note, Machine Learning]
categories: Note
---

## 分类与回归的区别

```markdown
|   特性   | 分类（监督学习） |           回归            |
| 输出类型  |    离散数据    |          连续数据          |
|   目的 	 |   寻找决策边界  |         找到最优拟合        |
| 评价方法  | 精度、混淆矩阵  | SSE(平方误差总和)、拟合优度   |
```

[转自知乎赵熙](https://www.zhihu.com/people/dataju)

**分类模型和回归模型本质一样，分类模型可将回归模型的输出离散化（下面例子1. 2. 4. 5.），回归模型也可将分类模型的输出连续化（下面例子3.）**

举几个例子:

1. Logistic Regression 和 Linear Regression：

2. 1. **Linear Regression**： 输出一个标量 wx+b，这个值是连续值，所以可以用来处理**回归**问题
   2. **Logistic Regression**：把上面的 wx+b 通过 sigmoid 函数映射到(0,1)上，并划分一个阈值，大于阈值的分为一类，小于等于分为另一类，可以用来处理**二分类**问题
   3. 更进一步：对于N分类问题，则是先得到N组w值不同的 wx+b，然后归一化，比如用 softmax 函数，最后变成N个类上的概率，可以处理**多分类**问题

3. Support Vector Regression 和 Support Vector Machine:

4. 1. **SVR**：输出 wx+b，即某个样本点到分类面的距离，是连续值，所以是**回归**模型
   2. **SVM**：把这个距离用 sign(·) 函数作用，距离为正(在超平面一侧)的样本点是一类，为负的是另一类，所以是**分类**模型

5. **Naive Bayes** 用于分类 和 回归:

6. 1. 用于分类：y是离散的类别，所以得到离散的 p(y|x)，给定 x ，输出每个类上的概率
   2. 用于回归：对上面离散的 p(y|x)求期望 ΣyP(y|x)，就得到连续值。但因为此时y本身是连续的值，所以最地道的做法是，得到连续的概率密度函数p(y|x)，然后再对y求期望。参考 [http://www.cs.waikato.ac.nz/~eibe/pubs/nbr.pdf](https://link.zhihu.com/?target=http%3A//www.cs.waikato.ac.nz/~eibe/pubs/nbr.pdf)

7. **前馈神经网络(如 CNN 系列)** 用于 分类 和 回归:

8. 1. 用于回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出可以看做向量 v，现全部连到一个神经元上，则这个神经元输出 wv+b，是一个连续值，可以处理回归问题，跟上面 Linear Regression 思想一样
   2. 用于N分类：现在这m个神经元最后连接到 N 个神经元，就有 N 组w值不同的 wv+b，同理可以归一化（比如用 softmax ）变成 N个类上的概率（补充一下，如果不用 softmax，而是每个 wx+b 用一个 sigmoid，就变成**多标签**问题，跟多分类的区别在于，样本可以被打上多个标签）

9. **循环神经网络(如 RNN 系列)** 用于分类 和 回归：

10. 1. 用于回归 和 分类： 跟 CNN 类似，输出层的值 y = wv+b，可做分类可做回归，只不过区别在于，RNN 的输出跟时间有关，即输出的是 {y(t), y(t+1),...}序列（关于时间序列，见下面的更新）

上面的例子其实都是从 prediction 的角度举例的，如果从 training 角度来看，分类模型和回归模型的目标函数不同，分类常见的是 log loss, hinge loss, 而回归是 square loss（关于 loss function，又是另一个story了，在此不展开了）

==== 进一步思考后的**重要更新，谈谈时间序列模型** ========

上面的例子 1~4 解决的是常见的分类/回归问题，而例5 解决的是 时间序列问题。

1. 上面例1~4 的模型只适用于：这些样本的 y，没有时间上的相关性，比如：

2. 1. 人脸识别（分类问题），输入 x 是人脸的图像矩阵，识别目标 y 是人的ID，离散值，显然人与人的ID没有时间上的关系
   2. 人脸年龄预测（回归问题），输入 x 还是人脸图像矩阵，识别目标 y 是人的年龄，连续值，显然人与人之间的年龄亦没有时间上的关系

3. 而当这些样本的 y 在时间上有相关性时，就变成了 时间序列问题，如果我们依然用非时间序列的方法来处理，就割裂了y的时间相关性，所以常见手段是用例5提到的RNN，（当然，还有 HMM, CRF 这些）但注意别用统计学里面那些愚蠢的 AR 模型（参考我的回答 [时间序列建模问题，如何准确的建立时间序列模型？ - 知乎用户的回答 - 知乎](https://www.zhihu.com/question/31833683/answer/152064596)）。应用场景：

4. 1. NLP 里的命名体识别（分类问题），输入是一句话，可以看做是由单词组成的时间序列（准确说是: 事件序列），输出是每个单词所属的标签
   2. 气温预测（回归问题），输入是历史时间的气温记录，输出是未来1天或多天的气温

总结一下，我认为，机器学习模型(有监督)本质是：

> 对一系列样本 ![(x,y)](https://www.zhihu.com/equation?tex=%28x%2Cy%29) 构建 ![f(x)\rightarrow y](https://www.zhihu.com/equation?tex=f%28x%29%5Crightarrow+y) 的映射

所以，对于时间序列问题，其实是构建一个 ![f(x_t,x_{t+1},...,x_{t+dt})\rightarrow y_{t+1},...,y_{t+dt+1}](https://www.zhihu.com/equation?tex=f%28x_t%2Cx_%7Bt%2B1%7D%2C...%2Cx_%7Bt%2Bdt%7D%29%5Crightarrow+y_%7Bt%2B1%7D%2C...%2Cy_%7Bt%2Bdt%2B1%7D) 的映射关系